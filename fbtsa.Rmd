---
title: "Feature-based time series analysis"
author: "Rob J Hyndman"
date: "21 June 2018"
fontsize: 14pt
output:
  beamer_presentation:
    fig_height: 5
    fig_width: 8
    highlight: tango
    incremental: no
    keep_tex: yes
    theme: metropolis
    includes:
      in_header: preamble.tex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, cache=TRUE,
  dev.args=list(bg=grey(0.9), pointsize=11))
library(Mcomp)
library(tsfeatures)
library(tidyverse)
library(data.table)
library(GGally)
source("functions.R")
source("ggbiplot.R")
set.seed(20180605)
```

# Time series feature spaces

## M3 competition
\full{M3paper}
\only<2>{
\placefig{1}{4}{height=3cm}{SMakridakis}
\placefig{8.5}{4}{height=3cm}{MHibon}}

## How to plot lots of time series?

```{r m3data}
scalem3 <- list()
for(i in 1:3003)
{
  scalem3[[i]] <- M3[[i]]$x-min(M3[[i]]$x)
  scalem3[[i]] <- as.numeric(scalem3[[i]]/max(scalem3[[i]]))
}

k <- sample(1:3003,3003)
for(i in 1:200)
{
  fname <- paste("M3data",i,sep="")
  savepdf(fname)
  plot(0,0,ylim=range(scalem3),xlim=c(0,1),xlab="Time",ylab="",type="n")
  for(i in 1:i)
    lines((1:length(scalem3[[k[i]]]))/length(scalem3[[k[i]]]), scalem3[[k[i]]])
  endpdf()
}
savepdf("M3data500")
plot(0,0,ylim=range(scalem3),xlim=c(0,1),xlab="Time",ylab="",type="n")
for(i in 1:500)
  lines((1:length(scalem3[[k[i]]]))/length(scalem3[[k[i]]]), scalem3[[k[i]]])
endpdf()
savepdf("M3data1000")
plot(0,0,ylim=range(scalem3),xlim=c(0,1),xlab="Time",ylab="",type="n")
for(i in 1:1000)
  lines((1:length(scalem3[[k[i]]]))/length(scalem3[[k[i]]]), scalem3[[k[i]]])
endpdf()
savepdf("M3data2000")
plot(0,0,ylim=range(scalem3),xlim=c(0,1),xlab="Time",ylab="",type="n")
for(i in 1:2000)
  lines((1:length(scalem3[[k[i]]]))/length(scalem3[[k[i]]]), scalem3[[k[i]]])
endpdf()
savepdf("M3dataall")
plot(0,0,ylim=range(scalem3),xlim=c(0,1),xlab="Time",ylab="",type="n")
for(i in 1:3003)
  lines((1:length(scalem3[[i]]))/length(scalem3[[i]]), scalem3[[i]])
endpdf()
```

\only<1>{\full{M3data1}}
\only<2>{\full{M3data2}}
\only<3>{\full{M3data3}}
\only<4>{\full{M3data4}}
\only<5>{\full{M3data5}}
\only<6>{\full{M3data10}}
\only<7>{\full{M3data20}}
\only<8>{\full{M3data30}}
\only<9>{\full{M3data40}}
\only<10>{\full{M3data50}}
\only<11>{\full{M3data100}}
\only<12>{\full{M3data200}}
\only<13>{\full{M3data500}}
\only<14>{\full{M3dataall}}


## Key idea
\placefig{9.1}{.5}{width=3.6cm}{tukey}
\begin{textblock}{3}(9.7,5.4)\small\textit{John W Tukey}\end{textblock}
\begin{textblock}{8}(0.7,1.2)
\begin{alertblock}{Cognostics}
Computer-produced diagnostics\\ (Tukey and Tukey, 1985).
\end{alertblock}
\end{textblock}\pause
\vspace*{2.5cm}

\alert{Examples for time series}

  * lag correlation
  * size and direction of trend
  * strength of seasonality
  * timing of peak seasonality
  * spectral entropy

\vspace*{0.3cm}
\begin{block}{}
Called ``features'' in the machine learning literature.
\end{block}



## An STL decomposition: N2096
\begin{alertblock}{}
\centerline{$Y_t = S_t + T_t + R_t$\qquad $S_{t}$ is periodic with mean 0}
\end{alertblock}

```{r stl, fig.height=4.7}
forecast::mstl(M3[["N2096"]]$x) %>%
  autoplot() + ylab("") + xlab("") +
  scale_x_continuous(breaks=seq(1982,1992,by=1), minor_breaks = NULL)
```

## Candidate features
\begin{block}{STL decomposition}
\centerline{$Y_t = S_t + T_t + R_t$}
\end{block}\pause\fontsize{13}{15}\sf

* Seasonal period
* Autocorrelations of data ($Y_1,\dots,Y_T$)
* Autocorrelations of data ($R_1,\dots,R_T$)
* Strength of seasonality: $\max\left(0,1 - \frac{\Var(R_t)}{\Var(Y_t-T_t)}\right)$
* Strength of trend:  $\max\left(0,1 - \frac{\Var(R_t)}{\Var(Y_t-S_t)}\right)$
* Spectral entropy: $H = - \int_{-\pi}^{\pi} f_y(\lambda) \log f_y(\lambda) d\lambda$, where $f_y(\lambda)$ is spectral density of $Y_t$.\\newline
Low values of $H$ suggest a time series that is easier to forecast (more signal).
* Optimal Box-Cox transformation of data

## tsfeatures package

```{r m3datalist, include=FALSE}
M3data <- purrr::map(Mcomp::M3,
  function(x){
      tspx <- tsp(x$x)
      ts(c(x$x,x$xx), start=tspx[1], frequency=tspx[3])
  })

```

\fontsize{9}{9}\sf

```{r ijf2017, echo=TRUE}
library(tsfeatures)
lambda_stl <- function(x,...) {
  lambda <- forecast::BoxCox.lambda(x,
    lower=0, upper=1, method='loglik')
  y <- forecast::BoxCox(x, lambda)
  c(stl_features(y,s.window='periodic', robust=TRUE, ...),
    lambda=lambda)
}
M3Features <- bind_cols(
  tsfeatures(M3data, c("frequency", "entropy")),
  tsfeatures(M3data, "lambda_stl", scale=FALSE)) %>%
  select(frequency, entropy, trend, seasonal_strength,
    e_acf1, lambda) %>%
  replace_na(list(seasonal_strength=0)) %>%
  rename(
    Frequency = frequency,
    Entropy = entropy,
    Trend = trend,
    Season = seasonal_strength,
    ACF1 = e_acf1,
    Lambda = lambda) %>%
  mutate(Period = as.factor(Frequency))
```

```{r M3examples, include=FALSE}
#Consider only long series
n <- unlist(lapply(M3,function(x){x$n}))
M3Featureslong <- M3Features[n>50,]
M3long <- M3[names(M3)[n>50]]
fnames <- c("M3Freq","M3spec","M3trend","M3season","M3acf","M3lambda")
k <- NROW(M3Featureslong)
for(i in 1:6)
{
  j <- order(M3Featureslong[[i]])
  savepdf(paste(fnames[i],"Lo",sep=""), width=20, height=7)
  print(autoplot(M3long[[j[1]]]$x) +
    ylab(M3long[[j[1]]]$sn) + xlab(""))
  endpdf()
  savepdf(paste(fnames[i],"Hi",sep=""), width=20, height=7)
  print(autoplot(M3long[[j[k]]]$x) +
    ylab(M3long[[j[k]]]$sn) + xlab(""))
  endpdf()
}
```

## Distribution of Period for M3

```{r M3period}
ggally_barDiag(M3Features,
               mapping = aes(Period), width=0.2,
               colour="#cc5900", fill="#cc5900")
```

## Distribution of Seasonality for M3

```{r M3season}
gghist(M3Features, aes_string("Season"))
```

\only<2->{
\begin{textblock}{6}(0.2,3)
  \begin{alertblock}{Low Seasonality}
    \includegraphics[width=6cm]{M3seasonLo.pdf}
  \end{alertblock}
\end{textblock}
}
\only<3>{
\begin{textblock}{6}(6.6,3)
  \begin{alertblock}{High Seasonality}
    \includegraphics[width=6cm]{M3seasonHi.pdf}
  \end{alertblock}
\end{textblock}
}

## Distribution of Trend for M3

```{r M3trend}
gghist(M3Features, aes_string("Trend"))
```

\only<2->{
\begin{textblock}{6}(0.2,3)
  \begin{alertblock}{Low Trend}
    \includegraphics[width=6cm]{M3trendLo.pdf}
  \end{alertblock}
\end{textblock}
}
\only<3>{
\begin{textblock}{6}(6.6,3)
  \begin{alertblock}{High Trend}
    \includegraphics[width=6cm]{M3trendHi.pdf}
  \end{alertblock}
\end{textblock}
}

## Distribution of Residual ACF1 for M3

```{r M3ACF1}
gghist(M3Features, aes_string("ACF1"))
```

\only<2->{
\begin{textblock}{6}(0.2,3)
  \begin{alertblock}{Low ACF1}
    \includegraphics[width=6cm]{M3acfLo.pdf}
  \end{alertblock}
\end{textblock}
}
\only<3>{
\begin{textblock}{6}(6.6,3)
  \begin{alertblock}{High ACF1}
    \includegraphics[width=6cm]{M3acfHi.pdf}
  \end{alertblock}
\end{textblock}
}

## Distribution of Spectral Entropy for M3

```{r M3entropy}
gghist(M3Features, aes_string("Entropy"))
```

\only<2->{
\begin{textblock}{6}(0.2,3)
  \begin{alertblock}{Low Entropy}
    \includegraphics[width=6cm]{M3specLo.pdf}
  \end{alertblock}
\end{textblock}
}
\only<3>{
\begin{textblock}{6}(6.6,3)
  \begin{alertblock}{High Entropy}
    \includegraphics[width=6cm]{M3specHi.pdf}
  \end{alertblock}
\end{textblock}
}

## Feature distributions

```{r ACF1SE}
ggplot(M3Features, aes(x=Entropy,y=ACF1)) + geom_point()
```

## Feature distributions

```{r TrendSE}
ggplot(M3Features, aes(x=Entropy,y=Trend)) + geom_point()
```

## Feature distributions

```{r ijf2017graphs, dependson="ijf2017"}
# Fig 1 of paper
yk_ggally_densityDiag <- wrap(gghist, adjust = 0.5)
yk_ggally_barDiag <-  wrap(ggally_barDiag, colour="#cc5900",
                           fill ="#cc5900", width = 0.2)
M3Features %>%
  select(Period, Entropy, Trend, Season, ACF1, Lambda) %>%
  ggpairs(diag = list(continuous = yk_ggally_densityDiag,
                      discrete = yk_ggally_barDiag),
          axisLabels = "none",
          lower=list(continuous = wrap("points", alpha = 0.5,  size=0.2))) -> p
print(p)
savepdf("PairwisePlot")
print(p)
endpdf()
```

## Feature distributions
\fontsize{11}{13}\sf

```r
M3Features %>%
  select(Period, Entropy, Trend, Season, ACF1, Lambda) %>%
  GGally::ggpairs()
```

\centerline{\includegraphics[width=10.4cm]{PairwisePlot}}

## Dimension reduction for time series

```{r m3sample, include=FALSE}
j <- sample(1:3003, 100)
ncol <- 5
n <- length(j)
savepdf("M3sample")
plot(0,0,ylim=c(0,n/ncol),xlim=c(0,ncol*1.2),yaxt="n",xaxt="n",ylab="",xlab="",bty="n",type="n")
for(i in 1:n)
  lines( (1:length(scalem3[[j[i]]]))/length(scalem3[[j[i]]]) + ((i-1)%%ncol)*1.1 ,
         scalem3[[j[i]]] + trunc((i-1)/ncol))
endpdf()
```

```{r m3pca, dependson="ijf2017"}
# 2-d Feature space (Top of Fig 2)
prcomp(select(M3Features, -Period), scale=TRUE)$x %>%
  as_tibble() %>%
  bind_cols(M3Features) %>%
  ggplot(aes(x=PC1, y=PC2)) + 
    geom_point() -> p
savepdf("FeatureSpace", height=13, width=13)
print(p)
endpdf()
```

\only<1->{\placefig{0}{1}{width=4cm,height=8.3cm,trim=0 0 200 0,clip=TRUE}{M3sample}}
\only<2->{\placefig{6}{1}{width=6cm}{PairwisePlot}}
\only<3>{\placefig{5.2}{5.2}{width=4cm}{FeatureSpace}}

\only<2->{\placefig{4}{2}{width=2cm}{arrow}}
\only<3>{\placefig{8.4}{4.2}{width=2cm,angle=-90}{arrow}}

\only<2->{\begin{textblock}{2.1}(4,2.6)
\begin{alertblock}{}\small
Feature calculation
\end{alertblock}
\end{textblock}}

\only<3->{\begin{textblock}{2.8}(9.7,4.1)
\begin{alertblock}{}\small
Principal component decomposition
\end{alertblock}
\end{textblock}}

## M3 feature space
\fontsize{12}{13}\sf

```r
prcomp(select(M3Features, -Period), scale=TRUE)$x %>%
  ggplot(aes(x=PC1, y=PC2))
```
\vspace*{-0.2cm}

```{r m3pca0, dependson="m3pca", fig.height=3.6, fig.width=3.6}
p
```


\begin{textblock}{1}(7,9.2)
\textcolor{black}{PC1}
\end{textblock}

\begin{textblock}{3}(9,3)
\begin{block}{}\fontsize{12}{13}\sf
First two PCs explain 58.5\% of the variance.
\end{block}
\end{textblock}

## M3 feature space

```{r m3biplot, dependson="m3pca", fig.width=4.5, fig.height=4.5}
prcomp(select(M3Features, -Period), scale=TRUE) %>%
 ggbiplot(alpha=0.2)
```

## M3 feature space

```{r m3pca1, dependson="m3pca", fig.width=5.5, fig.height=4.5}
 p + geom_point(aes(col=Period))
```


## Hyndman, Wang and Laptev (ICDM 2015)

```{r yahoo}
library(tsfeatures)
library(tidyverse)
library(anomalous)

# Compute the features used in Hyndman, Wang & Laptev (ICDM 2015).
# Note that crossing_points, peak and trough are defined differently
# in the tsfeatures package than in the Hyndman et al (2015) paper.
# Other features are the same. Using the real data from the paper
yahoo <- cbind(dat0, dat1, dat2, dat3)
hwl <- bind_cols(
         tsfeatures(yahoo,
           c("acf_features","entropy","lumpiness",
             "flat_spots","crossing_points")),
         tsfeatures(yahoo,"stl_features", s.window='periodic', robust=TRUE),
         tsfeatures(yahoo, "max_kl_shift", width=48),
         tsfeatures(yahoo,
           c("mean","var"), scale=FALSE, na.rm=TRUE),
         tsfeatures(yahoo,
           c("max_level_shift","max_var_shift"), trim=TRUE)) %>%
  select(mean, var, x_acf1, trend,
         seasonal_strength, peak, trough,
         entropy, lumpiness, spike, max_level_shift, max_var_shift, flat_spots,
         crossing_points, max_kl_shift, time_kl_shift)
```

```{r yahoo2, dependson="yahoo"}
# 2-d Feature space
prcomp(na.omit(hwl), scale=TRUE)$x %>%
  as_tibble() %>%
  ggplot(aes(x=PC1, y=PC2)) +
    geom_point()
```

# Irish smart metre data


```{r load}
load("DT.rda")
load("qdemand.rda")
load("jsdmat.rda")
source("functions.R")
```


## Irish smart metre data

\centerline{\includegraphics[width=1.18\linewidth]{SMARTGRID.jpg}}
  \vspace{-.85cm}
  \begin{flushright}
    { \tiny Figure: \url{http://solutions.3m.com}}
  \end{flushright}\vspace*{-0.4cm}\fontsize{12}{13}\sf

 * 500 households from smart metering trial
 * Electricity consumption at 30-minute intervals\newline between 14 July 2009 and 31 December 2010
 * Heating/cooling energy usage excluded

## Irish smart metre data

```{r timeplot1}
id <- 128
idlist <- unique(DT[,id])
if(id <= 500)
  id <- idlist[id]

# Subset of DT
j <- (DT[,id]==id)
z <- DT[j, ]
z$time <- z$day - 196 + z$period/48

ggplot(aes(y=demand, x=time), data=z) +
  geom_line() +
  ylab("Demand (kWh)") + xlab("Days") +
  ggtitle(paste("Demand for ID:",id)) +
  guides(fill=FALSE)
```

## Irish smart metre data

```{r timeplot2}
id <- 129
idlist <- unique(DT[,id])
if(id <= 500)
  id <- idlist[id]

# Subset of DT
j <- (DT[,id]==id)
z <- DT[j, ]
z$time <- z$day - 196 + z$period/48

ggplot(aes(y=demand, x=time), data=z) +
  geom_line() +
  ylab("Demand (kWh)") + xlab("Days") +
  ggtitle(paste("Demand for ID:",id)) +
  guides(fill=FALSE)
```

## Irish smart metre data

```{r timeplot3}
id <- 12
idlist <- unique(DT[,id])
if(id <= 500)
  id <- idlist[id]

# Subset of DT
j <- (DT[,id]==id)
z <- DT[j, ]
z$time <- z$day - 196 + z$period/48

ggplot(aes(y=demand, x=time), data=z) +
  geom_line() +
  ylab("Demand (kWh)") + xlab("Days") +
  ggtitle(paste("Demand for ID:",id)) +
  guides(fill=FALSE)
```


# Quantiles conditional on time of week

## Quantiles conditional on time of week
\fontsize{11}{13}\sf

* Compute sample quantiles at $p=0.01,0.02,\dots, 0.99$ for each household and each half-hour of the week.
* $336$ probability distributions per household.

```{r timeplot1repeat, fig.height=4.5}
id <- 128
idlist <- unique(DT[,id])
if(id <= 500)
  id <- idlist[id]

# Subset of DT
j <- (DT[,id]==id)
z <- DT[j, ]
z$time <- z$day - 196 + z$period/48

ggplot(aes(y=demand, x=time), data=z) +
  geom_line() +
  ylab("Demand (kWh)") + xlab("Days") +
  ggtitle(paste("Demand for ID:",id)) +
  guides(fill=FALSE)
```

## Quantiles conditional on time of week
\fontsize{11}{13}\sf

* Compute sample quantiles at $p=0.01,0.02,\dots, 0.99$ for each household and each half-hour of the week.
* $336$ probability distributions per household.


```{r qdemandplot1, fig.height=4.5}
qdemandplot(128)
```

## Quantiles conditional on time of week
\fontsize{11}{13}\sf

* Compute sample quantiles at $p=0.01,0.02,\dots, 0.99$ for each household and each half-hour of the week.
* $336$ probability distributions per household.

```{r timeplot2repeat, fig.height=4.5}
id <- 129
idlist <- unique(DT[,id])
if(id <= 500)
  id <- idlist[id]

# Subset of DT
j <- (DT[,id]==id)
z <- DT[j, ]
z$time <- z$day - 196 + z$period/48

ggplot(aes(y=demand, x=time), data=z) +
  geom_line() +
  ylab("Demand (kWh)") + xlab("Days") +
  ggtitle(paste("Demand for ID:",id)) +
  guides(fill=FALSE)
```

## Quantiles conditional on time of week
\fontsize{11}{13}\sf

* Compute sample quantiles at $p=0.01,0.02,\dots, 0.99$ for each household and each half-hour of the week.
* $336$ probability distributions per household.

```{r qdemandplot2, fig.height=4.5}
qdemandplot(129)
```

## Quantiles conditional on time of week
\fontsize{11}{13}\sf

* Compute sample quantiles at $p=0.01,0.02,\dots, 0.99$ for each household and each half-hour of the week.
* $336$ probability distributions per household.

```{r timeplot3repeat, fig.height=4.5}
id <- 12
idlist <- unique(DT[,id])
if(id <= 500)
  id <- idlist[id]

# Subset of DT
j <- (DT[,id]==id)
z <- DT[j, ]
z$time <- z$day - 196 + z$period/48

ggplot(aes(y=demand, x=time), data=z) +
  geom_line() +
  ylab("Demand (kWh)") + xlab("Days") +
  ggtitle(paste("Demand for ID:",id)) +
  guides(fill=FALSE)
```

## Quantiles conditional on time of week
\fontsize{11}{13}\sf

* Compute sample quantiles at $p=0.01,0.02,\dots, 0.99$ for each household and each half-hour of the week.
* $336$ probability distributions per household.

```{r qdemandplot3, fig.height=4.5}
qdemandplot(12)
```

## Quantiles conditional on time of week
\fontsize{12}{13}\sf

\centerline{\includegraphics[width=9.8cm]{quantileplot}}

> - Sample quantiles better than kernel density estimate:
\begin{itemize}
       \item presence of zeros
       \item non-negative support
       \item high skewness
\end{itemize}
> - Avoids missing data issues and variation in series length
> - Avoids timing of household events, holidays, etc.
> - Allows clustering of households based on probabilistic behaviour rather than coincident behaviour.
> - Allows identification of anomalous households.
> - Allows estimation of typical household behaviour.

# Finding typical and unusual households

## Pairwise distances

\placefig{0.1}{1.5}{width=5.5cm, height=1.8cm, keepaspectratio=false}{timeplot}
\placefig{7.2}{1.5}{width=5.5cm, height=1.8cm, keepaspectratio=false}{quantileplot}
\begin{textblock}{3}(5.7,2.2)\LARGE
$\longrightarrow$
\end{textblock}
\vspace*{.9cm}\fontsize{13}{15}\sf

 * The time series of $535\times48$ observations per household is mapped to a set of $7\times48\times99$ quantiles giving a bivariate surface for each household.

 * Can we compute pairwise distances between all households?

\placefig{0.1}{7.5}{width=5.5cm, height=1.8cm, keepaspectratio=false}{quantileplot}
\placefig{7.2}{7.5}{width=5.5cm, height=1.8cm, keepaspectratio=false}{quantile2plot}
\begin{textblock}{1.48}(5.65,8.2)
$\leftarrow\hfill~?\hfill\rightarrow$\\\fontsize{11}{12}\sf
\hfill Distance \hfill
\end{textblock}


## Jensen-Shannon distances

\begin{block}{Kullback-Leibler divergence between two densities}
$$D(p,q) = \int_{\infty}^\infty p(x) \log \frac{p(x)}{q(x)} dx$$
\vspace*{-0.2cm}\pause

Not symmetric: $D(p,q) \neq D(q,p)$
\end{block}
\pause

\begin{block}{Jensen-Shannon distance between two densities}
$$\text{JS}(p,q) =  [ D(p,r) +  D(q,r)] / 2\qquad\text{where~~}
r = (p+q)/2$$
\end{block}
\pause

\begin{block}{Distance between two households}
$$\Delta_{ij} = \sum_{t=1}^{7\times 48} \text{JS}(p_t,q_t)$$
\end{block}

## Kernel matrix and density ranking

\begin{block}{Similarity between two households}
$$
  w_{ij} = \exp(-\Delta_{ij}^2/h^2).
$$
\end{block}\pause

Row sums of the kernel matrix gives a scaled kernel density estimate of households:
\begin{block}{}
$$  \hat{f}_i = \sum_{j=1}^n w_{ij}$$
\end{block}

 * $h$ is bandwidth in Gaussian kernel.
 * Households can be ranked by density values.

## Typical households

```{r typical}
elecembed <- embedding(jsdmat, m=2)
# Look at modal observations
fxyhi <- kdedist(elecembed$distances, bandwidth=1e5)
mode1 <- order(fxyhi,decreasing=TRUE)[1]
mode2 <- order(fxyhi,decreasing=TRUE)[2]
mode3 <- order(fxyhi,decreasing=TRUE)[3]
qdemandplot(mode1)
```

## Typical households

```{r mode2}
qdemandplot(mode2)
```

## Typical households

```{r mode3}
qdemandplot(mode3)
```

## Anomalous households

```{r outlier1}
outlier1 <- order(fxyhi,decreasing=FALSE)[1]
qdemandplot(outlier1)
```

## Anomalous households

```{r outlier2}
outlier2 <- order(fxyhi,decreasing=FALSE)[2]
qdemandplot(outlier2)
```

## Anomalous households

```{r outlier3}
outlier3 <- order(fxyhi,decreasing=FALSE)[3]
qdemandplot(outlier3)
```

# Visualization via embedding

## Laplacian eigenmaps
\fontsize{13}{15}\sf

> - **Idea:** Embed conditional densities in a 2d space where the distances are preserved "as far as possible".
> -
 \begin{align*}
 \\[-1.36cm]
 \text{Let}\quad
   \bm{W} &=[w_{ij}] &&\text{~~ where~}w_{ij} = \exp(-\Delta_{ij}^2/h^2).\\
   \bm{D} &= \text{diag}(\hat{f}_i) &&\text{~~ where~}\hat{f}_i = \sum_{j=1}^n w_{ij}\\
   \bm{L} &=\bm{D}-\bm{W} &&\text{~~ (the Laplacian matrix).}\hspace{5cm}
 \end{align*}
> - Solve generalized eigenvector problem: $\bm{L}\bm{e} = \lambda \bm{D}\bm{e}$.
> - Let $\bm{e}_k$ be eigenvector corresponding to $k$th \emph{smallest} eigenvalue.
> - Then $\bm{e}_2$ and $\bm{e}_3$ create an embedding of households in 2d space.

## Key property of Laplacian embedding

Let $y_i = (e_{2,i},e_{3,i})$ be the embedded point corresponding to household $i$.

\begin{block}{}
Then the Laplacian eigenmap minimizes
$$
  \sum_{ij} w_{ij}(y_i-y_j)^2 = \bm{y}'\bm{L}\bm{y} \qquad\text{such that}\quad
\bm{y}'\bm{D}\bm{y}=1.
$$
\end{block}\pause

 > - the most similar points are as close as possible.
 > - First eigenvalue is 0 due to translation invariance.
 > - Equivalent to optimal embedding using Laplace-Beltrami operator on manifolds.


## Outliers computed in embedded space:

```{r}
#plot(elecembed, embedded=TRUE, noutliers=3) +
#  xlim(-2.7,1.9) +
#  ggtitle("Laplacian embedding (HDRs on embedded space)")
```

# Features and limitations

## Features and limitations
\fontsize{12}{13}\sf

\begin{block}{Features of approach}
\begin{itemize}
 \item Converting time series to quantile surfaces conditional on time of week.
 \item Using pairwise distances between households
 \item Using kernel matrices for density ranking, embedding and clustering
\end{itemize}
\end{block}\pause

**Unresolved issues**

* Need to select the bandwidth $h$ in constructing the similarity matrix.
* Two different uses of bandwidth: density-ranking, embedding. Different bandwidth in each case?
* The use of pairwise distances makes it hard to scale this algorithm.

